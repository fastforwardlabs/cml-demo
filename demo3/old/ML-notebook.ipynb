{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Airline ML\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "flight_df=spark.read.parquet(\n",
    "  \"s3a://ml-field/demo/flight-analysis/data/airline_parquet_2/\",\n",
    ")\n",
    "\n",
    "flight_df = flight_df.na.drop().limit(10000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.97.131.9:20049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.7.0.0.0-334</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://172.20.0.1:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Airline ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8cddf41cc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf,substring\n",
    "\n",
    "convert_time_to_hour = udf(lambda x: x if len(x) == 4 else \"0{}\".format(x),StringType())\n",
    "#df.withColumn('COLUMN_NAME_fix',udf1('COLUMN_NAME')).show()\n",
    "\n",
    "flight_df = flight_df.withColumn('CRS_DEP_HOUR', substring(convert_time_to_hour(\"CRS_DEP_TIME\"),0,2))\n",
    "flight_df = flight_df.withColumn('CRS_ARR_HOUR', substring(convert_time_to_hour(\"CRS_ARR_TIME\"),0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "numeric_cols = [\"CRS_ELAPSED_TIME\",\"DISTANCE\"]\n",
    "\n",
    "op_carrier_indexer = StringIndexer(inputCol ='OP_CARRIER', outputCol = 'OP_CARRIER_INDEXED',handleInvalid=\"keep\")\n",
    "op_carrier_encoder = OneHotEncoder(inputCol ='OP_CARRIER_INDEXED', outputCol='OP_CARRIER_ENCODED')\n",
    "\n",
    "# op_carrier_fl_num_indexer = StringIndexer(inputCol ='OP_CARRIER_FL_NUM', outputCol = 'OP_CARRIER_FL_NUM_INDEXED',handleInvalid=\"keep\")\n",
    "# op_carrier_fl_num_encoder = OneHotEncoder(inputCol ='OP_CARRIER_FL_NUM_INDEXED', outputCol='OP_CARRIER_FL_NUM_ENCODED')\n",
    "\n",
    "origin_indexer = StringIndexer(inputCol ='ORIGIN', outputCol = 'ORIGIN_INDEXED',handleInvalid=\"keep\")\n",
    "origin_encoder = OneHotEncoder(inputCol ='ORIGIN_INDEXED', outputCol='ORIGIN_ENCODED')\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol ='DEST', outputCol = 'DEST_INDEXED',handleInvalid=\"keep\")\n",
    "dest_encoder = OneHotEncoder(inputCol ='DEST_INDEXED', outputCol='DEST_ENCODED')\n",
    "\n",
    "crs_dep_hour_indexer = StringIndexer(inputCol ='CRS_DEP_HOUR', outputCol = 'CRS_DEP_HOUR_INDEXED',handleInvalid=\"keep\")\n",
    "crs_dep_hour_encoder = OneHotEncoder(inputCol ='CRS_DEP_HOUR_INDEXED', outputCol='CRS_DEP_HOUR_ENCODED')\n",
    "\n",
    "# crs_arr_hour_indexer = StringIndexer(inputCol ='CRS_ARR_HOUR', outputCol = 'CRS_ARR_HOUR_INDEXED',handleInvalid=\"keep\")\n",
    "# crs_arr_hour_encoder = OneHotEncoder(inputCol ='CRS_ARR_HOUR_INDEXED', outputCol='CRS_ARR_HOUR_ENCODED')\n",
    "\n",
    "input_cols=[\n",
    "    'OP_CARRIER_ENCODED',\n",
    "    #'OP_CARRIER_FL_NUM_ENCODED',\n",
    "    'ORIGIN_ENCODED',\n",
    "    'DEST_ENCODED',\n",
    "    'CRS_DEP_HOUR_ENCODED'] + numeric_cols\n",
    "    #'CRS_ARR_HOUR_ENCODED'] + numeric_cols\n",
    "\n",
    "\n",
    "# input_cols=[\n",
    "#     'OP_CARRIER_ENCODED',\n",
    "#     'OP_CARRIER_FL_NUM_ENCODED',\n",
    "#     'ORIGIN_ENCODED',\n",
    "#     'DEST_ENCODED',\n",
    "#     'CRS_DEP_HOUR_ENCODED',\n",
    "#     'CRS_ARR_HOUR_ENCODED'] + numeric_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = input_cols,\n",
    "    outputCol = 'features')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#                                     , \n",
    "#                                     numTrees = param_numTrees, \n",
    "#                                     maxDepth = param_maxDepth,  \n",
    "#                                     impurity = param_impurity)\n",
    "\n",
    "pipeline = Pipeline(stages=[op_carrier_indexer, \n",
    "                            op_carrier_encoder, \n",
    "                            #op_carrier_fl_num_indexer,\n",
    "                            #op_carrier_fl_num_encoder,\n",
    "                            origin_indexer,\n",
    "                            origin_encoder,\n",
    "                            dest_indexer,\n",
    "                            dest_encoder,\n",
    "                            crs_dep_hour_indexer,\n",
    "                            crs_dep_hour_encoder,\n",
    "                            #crs_arr_hour_indexer,\n",
    "                            #crs_arr_hour_encoder,\n",
    "                            assembler])\n",
    "\n",
    "pipelineModel = pipeline.fit(flight_df)\n",
    "model_df = pipelineModel.transform(flight_df)\n",
    "selectedCols = ['CANCELLED', 'features']# + cols\n",
    "model_df = model_df.select(selectedCols)\n",
    "model_df.printSchema()\n",
    "(train, test) = model_df.randomSplit([0.7, 0.3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'CANCELLED', maxIter=10)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711228023866633"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainingSummary = lrModel.summary\n",
    "# roc = trainingSummary.roc.toPandas()\n",
    "# plt.plot(roc['FPR'],roc['TPR'])\n",
    "# plt.ylabel('False Positive Rate')\n",
    "# plt.xlabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()\n",
    "# print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "predictionslr = lrModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionslr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfclassifier = RandomForestClassifier(labelCol = 'CANCELLED', \n",
    "                                    featuresCol = 'features')\n",
    "rfmodel = rfclassifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5981694141610938"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictionsrf = rfmodel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10,featuresCol = 'features', labelCol = 'CANCELLED')\n",
    "\n",
    "gbtModel = gbt.fit(train)\n",
    "\n",
    "predictions = gbtModel.transform(test)\n",
    "#predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5584493086849524"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsgbt = gbtModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsgbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
