from surprise import SVD, Dataset, Reader
import joblib
import pandas
import numpy
from surprise.model_selection import cross_validate

# First, load the dataframe generated by Spark from disk.
local_df = joblib.load('clickstream.pkl')

ips = local_df['ip'].unique()
ip_idx = dict(zip(ips, np.arange(0,len(ips))))
products = local_df['product'].unique()
product_idx = dict(zip(products, np.arange(0,len(products))))

# Now transform it to the format expected by the Python
# recommendation package 'surprise'. The current data
# has columns 'product', 'ip', 'date_logged' and 'url'.
# Surprise requires columns corresponding to user id,
# item id and rating in that order.

grouped_series = local_df.groupby(['ip', 'product']).size()
ratings_dict = {
  "userId": [idx[0] for idx in grouped_series.index],
  "itemId": [idx[1] for idx in grouped_series.index],
  "rating": list(grouped_series)
}
surprise_df = pandas.DataFrame(ratings_dict, columns=['userId', 'itemId', 'rating'])

# Load the dataframe into a surprise Dataset object
reader = Reader(rating_scale=(1, surprise_df['rating'].max()))
data = Dataset.load_from_df(surprise_df, reader)

# We'll use the famous SVD algorithm to train the
# recommender.
algo = SVD()
trainset = data.build_full_trainset()
algo.fit(trainset)

# Save the trained recommender to disk so we can deploy
# the predictor as a model.
joblib.dump(algo, 'recommender.pkl')